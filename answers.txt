Questions:
1. Can you get better accuracies on the language identication task using the multi-layer perceptron?

2. Switch the feature set of the language identication from letter-bigrams to letter-unigrams (single letters). What's the best you can do with the log-linear model with these features?
What's the best you can do with the MLP?

3. Verify that your MLP can learn the XOR function (you can see a training-set for XOR in the file xor_data.py). How many iterations does it take to correctly solve xor?

Answers:
1. No, in our tests we never got better accuracies than the log-linear model (88% accuracy on dev set).
We tried different numbers of hidden layers and different numbers of nodes in each layer, but the best we could do was 88% accuracy on the dev set.
We assume that using techniques like dropout and regularization would help, but the instructions for the assignment did not mention these techniques.

2. After trying different combinations of hyper-paramethers both models got at most 68% accuracy on the dev set.

3. It took anywhere between 40 and 200+ iteration to correctly solve XOR.
 in our opinion it's because of the random initialization of the weights and the random order of the training examples.